# YouTube to Open Notebook - Schnellstart (5 Minuten)

## ‚ö° Automatisches Setup (Empfohlen!)

**Du musst KEINE Spalten manuell anlegen!**

### Methode A: SQL-Script (1 Befehl) üöÄ

**Auf dem Server:**

```bash
# 1. Updates holen
cd ~/ai/ai-launchkit-local
git pull origin main

# 2. SQL-Script ausf√ºhren (erstellt beide Tables automatisch)
cat n8n_custom_workflows/youtube-open-notebook/setup/create_tables.sql | \
  docker exec -i postgres psql -U postgres -d postgres

# 3. Verifiziere
docker exec postgres psql -U postgres -d postgres -c "
  SELECT table_name 
  FROM information_schema.tables 
  WHERE table_name IN ('youtube_channels', 'youtube_videos');"
```

**Erwartete Ausgabe:**
```
    table_name     
-------------------
 youtube_channels
 youtube_videos
(2 rows)

‚úÖ Tables erstellt!
‚úÖ 2 Beispiel-Channels eingef√ºgt!
```

**Das war's!** Keine manuelle Spalten-Erstellung n√∂tig!

---

### Methode B: Copy-Paste in n8n UI (10 Minuten)

**Falls du die UI bevorzugst:**

1. √ñffne n8n: `http://192.168.178.151:8000`
2. Gehe zu **Workflows** ‚Üí **Tables**
3. Klicke **"Create New Table"**

#### Table 1: youtube_channels (10 Spalten)

| Spalte | Typ | Einstellungen |
|--------|-----|---------------|
| channel_id | Text | ‚úÖ Required, ‚úÖ Unique |
| channel_name | Text | ‚úÖ Required |
| channel_url | Text | ‚úÖ Required |
| original_language | Text | ‚úÖ Required |
| enabled | Boolean | ‚úÖ Required, Default: true |
| notebook_name | Text | ‚úÖ Required |
| notebook_id | Text | ‚ùå Optional |
| last_sync | DateTime | ‚ùå Optional |
| video_count | Number | ‚ùå Optional, Default: 0 |
| created_at | DateTime | ‚ùå Auto-create |

#### Table 2: youtube_videos (15 Spalten)

| Spalte | Typ | Einstellungen |
|--------|-----|---------------|
| video_id | Text | ‚úÖ Required, ‚úÖ Unique |
| channel_id | Text | ‚úÖ Required |
| title | Text | ‚úÖ Required |
| url | Text | ‚úÖ Required |
| duration_seconds | Number | ‚úÖ Required |
| published_date | DateTime | ‚úÖ Required |
| thumbnail_url | Text | ‚ùå Optional |
| detected_language | Text | ‚ùå Optional |
| needs_translation | Boolean | ‚ùå Optional |
| status | Select | ‚úÖ Required, Default: "discovered" |
| skip_reason | Text | ‚ùå Optional |
| notebook_entry_url | Text | ‚ùå Optional |
| discovered_at | DateTime | ‚úÖ Auto-create |
| processed_at | DateTime | ‚ùå Optional |
| error_message | Text | ‚ùå Optional |

**Status Select-Optionen:**
`discovered`, `transcribing`, `translating`, `summarizing`, `podcast_generating`, `completed`, `failed`, `skipped`

---

## üöÄ Workflow Installation

### Schritt 1: PostgreSQL Credentials in n8n erstellen

**Wichtig:** Der Workflow ben√∂tigt PostgreSQL Credentials!

**In n8n Web UI:**
1. Gehe zu **Settings** ‚Üí **Credentials** ‚Üí **Add Credential**
2. W√§hle **"Postgres"**
3. Konfiguriere:
   ```
   Name: AI LaunchKit PostgreSQL
   Host: postgres
   Database: postgres
   User: postgres
   Password: [Dein POSTGRES_PASSWORD aus .env]
   Port: 5432
   SSL: Disabled
   ```
4. **Test Connection** klicken
5. **Save** klicken

**Hinweis:** Passwort findest du in deiner `.env` Datei:
```bash
grep POSTGRES_PASSWORD ~/ai/ai-launchkit-local/.env
```

### Schritt 2: Workflow importieren

```bash
# Tables sollten bereits existieren (via SQL-Script oben)

# In n8n Web UI:
# 1. Klicke "+ Add Workflow"
# 2. Men√º ‚Üí "Import from File"  
# 3. W√§hle: n8n_custom_workflows/youtube-open-notebook/workflows/01-youtube-channel-sync-mvp.json
# 4. Workflow √∂ffnet sich
```

### Schritt 2: Ersten Test ausf√ºhren

```bash
# 1. Im Workflow: Klicke "Execute Workflow"
# 2. Beobachte Logs
# 3. Pr√ºfe Table youtube_videos
# 4. √ñffne Open Notebook: http://192.168.178.151:8100
```

**Das war's! Workflow l√§uft!** ‚úÖ

---

## üéØ Kompletter 5-Minuten-Setup

```bash
# Auf dem Server (192.168.178.151)

# 1. Updates holen (30 Sekunden)
cd ~/ai/ai-launchkit-local
git pull origin main

# 2. PostgreSQL Settings anpassen (1 Minute)
nano .env
# F√ºge hinzu:
# POSTGRES_MAX_CONNECTIONS=500
# POSTGRES_SHARED_BUFFERS=24GB
# POSTGRES_EFFECTIVE_CACHE_SIZE=72GB
# POSTGRES_WORK_MEM=256MB
# POSTGRES_MAX_WORKER_PROCESSES=16
# N8N_WORKER_COUNT=8

# 3. Stack neu starten (2 Minuten)
docker compose -p localai -f docker-compose.local.yml down
docker compose -p localai -f docker-compose.local.yml up -d
sleep 30

# 4. Tables erstellen (10 Sekunden)
cat n8n_custom_workflows/youtube-open-notebook/setup/create_tables.sql | \
  docker exec -i postgres psql -U postgres -d postgres

# 5. Workflow importieren (1 Minute)
# ‚Üí In n8n UI: Import workflows/01-youtube-channel-sync-mvp.json

# 6. Test ausf√ºhren (30 Sekunden)
# ‚Üí In n8n UI: Execute Workflow

# ‚úÖ FERTIG! (Total: ~5 Minuten)
```

---

## üìã Checkliste

- [ ] Git pull ausgef√ºhrt
- [ ] .env angepasst (PostgreSQL Settings)
- [ ] Stack neu gestartet
- [ ] SQL-Script ausgef√ºhrt (Tables erstellt)
- [ ] Workflow importiert
- [ ] Ersten Test durchgef√ºhrt
- [ ] Open Notebook gepr√ºft (Port 8100)

---

## üîß Troubleshooting

### Problem: SQL-Script Fehler

```bash
# Hinweis: n8n nutzt die 'postgres' Datenbank in AI LaunchKit
# Pr√ºfe ob Tables bereits existieren
docker exec postgres psql -U postgres -d postgres -c "\dt"
```

### Problem: Tables existieren bereits

```bash
# L√∂schen und neu erstellen
docker exec postgres psql -U postgres -d postgres -c "
  DROP TABLE IF EXISTS youtube_videos CASCADE;
  DROP TABLE IF EXISTS youtube_channels CASCADE;"

# Dann SQL-Script erneut ausf√ºhren
```

### Problem: Workflow findet Tables nicht

```bash
# Pr√ºfe Tabellen-Namen
docker exec postgres psql -U postgres -d postgres -c "\dt"

# Sollte zeigen:
#  youtube_channels
#  youtube_videos
```

---

## üí° Zusatz-Features (Optional)

### Channel hinzuf√ºgen (via SQL)

```bash
docker exec postgres psql -U postgres -d postgres -c "
INSERT INTO youtube_channels (
  channel_id, 
  channel_name, 
  channel_url, 
  original_language, 
  enabled, 
  notebook_name
) VALUES (
  'DEINE_CHANNEL_ID',
  'Kanal Name',
  'youtube.com/@username',
  'en',
  true,
  'YT: Kanal Name'
);"
```

### Status aller Videos anzeigen

```bash
docker exec postgres psql -U postgres -d postgres -c "
SELECT status, count(*) 
FROM youtube_videos 
GROUP BY status;"
```

### Fehlgeschlagene Videos auflisten

```bash
docker exec postgres psql -U postgres -d postgres -c "
SELECT video_id, title, error_message 
FROM youtube_videos 
WHERE status = 'failed';"
```

---

## üéâ Fertig!

**Mit dem SQL-Script:**
- ‚úÖ Keine manuelle Spalten-Erstellung
- ‚úÖ 1 Befehl, alles fertig
- ‚úÖ 2 Beispiel-Channels bereits drin
- ‚úÖ Indexes automatisch erstellt

**Workflow ist produktionsbereit!** üöÄ

---

**Erstellt:** 8.11.2025
**Autor:** AI LaunchKit Community
**Support:** docs/README.md
